<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>翻译：ImageNet Classification with Deep Convolutional Neural Networks | asdfv1929</title>
    <meta name="description" content="This is asdfv1929 &#39;s  blog">
    <meta name="generator" content="VuePress 1.4.0">
    <link rel="preload" href="/assets/css/0.styles.b7bc5868.css" as="style"><link rel="preload" href="/assets/js/app.806dfd1e.js" as="script"><link rel="preload" href="/assets/js/7.97edb3cd.js" as="script"><link rel="preload" href="/assets/js/17.84ae493d.js" as="script"><link rel="prefetch" href="/assets/js/1.a7200042.js"><link rel="prefetch" href="/assets/js/10.319aaec7.js"><link rel="prefetch" href="/assets/js/11.4bb85e64.js"><link rel="prefetch" href="/assets/js/12.aeabf8ce.js"><link rel="prefetch" href="/assets/js/13.1994c1f4.js"><link rel="prefetch" href="/assets/js/14.8d0c1330.js"><link rel="prefetch" href="/assets/js/15.e9b3d716.js"><link rel="prefetch" href="/assets/js/16.e56fbcb8.js"><link rel="prefetch" href="/assets/js/18.2e24a23c.js"><link rel="prefetch" href="/assets/js/19.a4f10f64.js"><link rel="prefetch" href="/assets/js/20.8798d2eb.js"><link rel="prefetch" href="/assets/js/21.3f4b98ee.js"><link rel="prefetch" href="/assets/js/22.7f8cc741.js"><link rel="prefetch" href="/assets/js/23.3838effa.js"><link rel="prefetch" href="/assets/js/24.95a7f706.js"><link rel="prefetch" href="/assets/js/25.9a7b8b82.js"><link rel="prefetch" href="/assets/js/26.c4136c20.js"><link rel="prefetch" href="/assets/js/27.3d7c8852.js"><link rel="prefetch" href="/assets/js/28.481d0563.js"><link rel="prefetch" href="/assets/js/29.26689871.js"><link rel="prefetch" href="/assets/js/3.123d801a.js"><link rel="prefetch" href="/assets/js/30.f05c5529.js"><link rel="prefetch" href="/assets/js/31.bf5213a0.js"><link rel="prefetch" href="/assets/js/32.d7d1f8fa.js"><link rel="prefetch" href="/assets/js/33.0c34af6b.js"><link rel="prefetch" href="/assets/js/34.d10a7934.js"><link rel="prefetch" href="/assets/js/35.adb4d6bc.js"><link rel="prefetch" href="/assets/js/36.58b0719b.js"><link rel="prefetch" href="/assets/js/37.08b9b138.js"><link rel="prefetch" href="/assets/js/38.4d490636.js"><link rel="prefetch" href="/assets/js/39.7917ef35.js"><link rel="prefetch" href="/assets/js/4.d52bea1e.js"><link rel="prefetch" href="/assets/js/40.91d56082.js"><link rel="prefetch" href="/assets/js/41.1618df15.js"><link rel="prefetch" href="/assets/js/42.7a8a74c9.js"><link rel="prefetch" href="/assets/js/43.438bbe30.js"><link rel="prefetch" href="/assets/js/44.27125ea0.js"><link rel="prefetch" href="/assets/js/5.6e68ab49.js"><link rel="prefetch" href="/assets/js/6.849e5b0c.js"><link rel="prefetch" href="/assets/js/8.5321f08d.js"><link rel="prefetch" href="/assets/js/9.4568eb7b.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b7bc5868.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="vuepress-theme-meteorlxy"><header class="header" data-v-7a046aea><div data-v-e4145d0a data-v-7a046aea><nav class="navbar" data-v-e4145d0a><div class="container" data-v-e4145d0a><a href="/" class="router-link-active" data-v-e4145d0a><span class="navbar-site-name" data-v-e4145d0a>
          asdfv1929
        </span></a> <div class="navbar-toggler" data-v-e4145d0a><svg class="icon" style="font-size:1.2em;" data-v-e4145d0a data-v-e4145d0a><title data-v-e4145d0a data-v-e4145d0a>menu</title><use xlink:href="#icon-menu" data-v-e4145d0a data-v-e4145d0a></use></svg></div> <div class="navbar-links" data-v-e4145d0a><a href="/" class="navbar-link" data-v-e4145d0a>
            Home
          </a><a href="/posts/" class="navbar-link router-link-active" data-v-e4145d0a>
            Posts
          </a><a href="/links/" class="navbar-link" data-v-e4145d0a>
            Links
          </a><a href="/about/" class="navbar-link" data-v-e4145d0a>
            About
          </a></div></div></nav> <div class="navbar-holder" style="display:none;" data-v-e4145d0a></div></div> <div class="banner" data-v-98d6aa8c data-v-7a046aea data-v-7a046aea><div class="container" data-v-98d6aa8c><div class="center" data-v-98d6aa8c><h1 data-v-98d6aa8c data-v-7a046aea>
          翻译：ImageNet Classification with Deep Convolutional Neural Networks
        </h1></div></div></div></header> <div class="container clearfix show-aside" data-v-4dd605a1 data-v-4dd605a1><main class="main" data-v-4dd605a1><div class="post" data-v-4dd605a1 data-v-4dd605a1><section class="post-meta main-div" data-v-4e23451f><section class="post-date clearfix" data-v-4e23451f><span class="create-date" data-v-4e23451f>
      Created : 2017-12-31
    </span> <!----></section> <section class="post-links" data-v-4e23451f><a href="/posts/2017/11/19/next-hexo-blog.html" class="post-link" data-v-4e23451f>
      Previous Post : 利用GitHub+Node.js+Hexo搭建个人博客
    </a> <a href="/posts/2018/01/01/life-new-year-2018.html" class="post-link" data-v-4e23451f>
      Next Post : Happy New Year
    </a></section></section> <article class="main-div"><div class="post-content content content__default"><p>此为本人在学习AlexNet时，将论文翻译成中文而作，以翻促学。<br> 
论文地址：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</p> <p>注：<br>
本人也是第一次翻译外文论文，若有纰漏之处，敬请谅解，并可邮箱通知于我[asdfv1929#163.com]进行修改；<br>
翻译内容仅供学习使用。</p> <table><thead><tr><th>Name</th> <th>DESC</th></tr></thead> <tbody><tr><td>TITLE</td> <td>ImageNet Classification with Deep Convolutional Neural Networks</td></tr> <tr><td>AUTHOR</td> <td>Alex Krizhevsky</td></tr> <tr><td>PBYEAR</td> <td>2012</td></tr> <tr><td>TRANSLATION</td> <td>asdfv1929</td></tr> <tr><td>TIME</td> <td>2017.12</td></tr></tbody></table> <p>正文翻译：</p> <h1 id="摘要"><a href="#摘要" class="header-anchor">#</a> 摘要</h1> <p>我们训练了一个大规模的深度卷积神经网络去将ImageNet LSVRC-2010比赛中的120万张高清图像划分到1000个不同的类别中。在测试数据上，我们将top-1和top-5的误差率分别降到了37.5%和17.0%，这比之前的技术水平要好得多。该神经网络，拥有6千万个参数（parameters）和65万个神经元（neurons），包含有5个卷积层（convolutional layers），其中一些卷积层的后面跟着最大池化层（max-pooling layers），还有3个全连接层（full-connected layers）以及一个最后的1000-way softmax函数。为了让训练速度更快，我们采用了非饱和神经元（non-saturating neurons）和一种卷积操作的高效GPU实现方法。为减少全连接层中的过拟合，我们采用了一个最近研究出来的正则化方法（regularization method），叫“dropout”，它被证明是十分有效的。我们也用该神经网络模型的一个变种去参加了ILSVRC-2012比赛，并且同第二名的top-5误差率26.2%相比，我们以top-5误差率15.3%获得了冠军。</p> <h1 id="引言"><a href="#引言" class="header-anchor">#</a> 引言</h1> <p>当前的目标识别方法主要都是利用了机器学习方法。为了提高他们的性能，我们可以收集更大规模的数据集，学习更多强大的模型，采用更优的技术来防止过拟合（overfitting）。直到最近，带有标签（label）的图像数据集的规模仍是相对较小，一般是在万张数量级上（例如NORB，Caltech-101/256，和CIFAR-10/100）。我们可以在这种规模大小的数据集上做简单的识别任务，特别是当它们通过标签保存转换（label-preserving transformations）方法增强了数据。举例来说，当前在MNIST数字识别任务上的最优误差率（&lt;0.3%）已接近人的表现。但是现实环境中的目标对象（object）有着相当大的变化性，所以若要学习去识别它们，就有必要使用更大规模的训练集。事实上，小规模图像数据集的缺点早已被公认，但到最近才可能收集到数百万张带标签的图像数据集。这些新的更大规模的数据集包括有LabelMe，它由数十万张全分割（full-segmented）的图像组成，还有ImageNet，它由超过1500万张带有label的高清图像组成，这些图像有超过22000个种类。</p> <p>为了从百万张图像中学习数千种目标，我们需要一个具有较大学习能力的模型。然而，目标识别任务的巨大复杂性意味着即使像ImageNet这等规模大的数据集也不能确定该问题，所以我们的模型应采用许多先验知识来弥补我们所没有的数据。卷积神经网络（CNNs）便构成了这样的一类模型。它们的能力可以通过改变它们的深度（depth）和广度（breadth）来控制，并且它们对图像的性质（即，统计上的稳定性和像素依赖的局部性stationarity of statistics and locality of pixel dependencies）也能做出强大且大多正确的假设（预测）。因此，同具有相似规模层（similarly-sized layers）的标准前馈（feedforward）神经网络相比，CNNs拥有更少的连接和参数，因此它们更易于训练，而且它们理论上的最优性能可能仅比前馈神经网络稍差一些。</p> <p>尽管CNNs具有吸引人的一些特性，而且对于本地架构（local architecture）非常的高效，但它们在大规模应用于高分辨率图像方面上仍然过于昂贵。幸运的是，当前的GPUs，加上二维卷积的高度优化实现方法，足以促进有趣的CNNs的训练，最近的数据集比如说ImageNet，包含了足够的标记样本（labeled examples）来训练此类模型，且不会出现严重的过拟合问题。</p> <p>本文的主要贡献如下所示：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了最大的卷积神经网络之一，并且在这些数据集上获得了迄今为止最好的结果。我们编写了一个二维卷积的高度优化的GPU实现方法，以及其他所有在训练神经网络过程中固有的一些操作，这些我们都公开提供（ http://code.google.com/p/cuda-convnet/ ）。我们的网络包含了一些新的和不寻常的特点，它们可以提高网络的性能，缩短训练时间，具体内容见 Section 3。即使是拥有120万个带标记的训练样本，网络的大小仍然会使得过拟合（overfitting）成为一个严重的问题，所以我们使用了几个有效的技术来防止过拟合，具体信息将在Section 4中介绍。我们最终的网络包含了5个卷积层和3个全连接层，其中深度看上去很重要：我们发现，若移除任意一个卷积层（每个卷积层仅包含不到1%的模型参数）均会导致性能变差。</p> <p>最后，网络的规模大小主要受限于当前GPUs的可用存储量以及我们能接受的训练时间。我们的神经网络在两台GTX 580 3GB GPUs上训练花费了5至6天的时间。我们所有的实验均表明，只要有更快的GPUs和更大的数据集，我们的实验结果就能进一步提高。</p> <h1 id="数据集"><a href="#数据集" class="header-anchor">#</a> 数据集</h1> <p>ImageNet是一个拥有超过1500万张带标签的高分辨率图像的数据集，且这些图像大致归属于22000个类别。这些图像收集自网络，并由human labelers使用Amazon的Mechanical Turk crowd-sourceing tool进行人工标记。从2010年开始，作为Pascal Visual Object Challenge的一部分，一个叫ImageNet Large-Scale Visual Recognition（ILSVRC）的比赛开始举办。ILSVRC使用了ImageNet的子集，这个子集中包含了1000个类别，每个类别大约有1000张图像。总之，这个子集大概有120万张训练图像，5万张验证图像，以及15万张测试图像。</p> <p>ILSVRC-2010是ILSVRC中测试集labels可获得的唯一版本，因此我们是在这个数据集上做了大多数实验。我们也用我们的模型参加了ILSVRC-2012比赛，在Section 6我们会展示关于这个数据集（2012）的实验结果，但其测试集的labels不可获得。在ImageNet中，通常检验这两类误差率：top-1和top-5，其中top-5误差率表示测试图像中的正确label不在模型所认为的可能性最大的5个labels当中的占比。</p> <p>ImageNet包含了各种分辨率的图像，而我们的系统要求输入数据的维度恒定（constant input dimensionality）。因此，我们对图像进行下采样（down-sample）到一个固定的分辨率 256 x 256。给定一个矩形图像，我们首先缩放图像使得图像的短边长度为256，之后从结果图像中裁剪出中心 256 x 256 大小的块。我们并未使用任何其他方法对图像进行预处理，除了从每个像素中减去训练集的平均活动（subtracting the mean activity over the training set from each pixel）。因此我们是在像素的原始RGB值上训练我们的网络模型。</p> <h1 id="架构"><a href="#架构" class="header-anchor">#</a> 架构</h1> <p>我们网络的架构在图2中总结展示出。它包含了8个学习层--5个卷积层和3个全连接层。接下来，我们将介绍我们网络的架构中几个新的不寻常的特点。Sections 3.1-3.4按照我们对它们重要性的评估进行排序，最重要的排在前列。</p> <h2 id="relu-非线性（nonlinearity）"><a href="#relu-非线性（nonlinearity）" class="header-anchor">#</a> ReLU 非线性（Nonlinearity）</h2> <p>对神经元输出 f 作为其输入 x 的函数的标准建模方法是 f(x) = tanh(x) 或 f(x) = (1 + e^-x)^-1 。从采用梯度下降（gradient descent）方法的训练时间来看，这些饱和非线性（saturating nonlinearities）是比非饱和非线性（non-saturating nonlinearity）f(x) = max(0, x) 要慢得多。根据Nair和Hinton的想法，我们把具有这种非线性的神经元称为整流线性单元Rectified Linear Units （ReLUs）。使用ReLUs的深度卷积神经网络比使用单元的网络训练速度快上几倍。这在Figure 1中可以看到，上面展示了在一个特定4层卷积网络上对CIFAR-10数据集的训练误差率降到25%所需要的迭代次数。这幅图表明，如果我们使用的是传统的饱和神经元模型，我们将不能够训练出如此大规模的神经网络。</p> <p><img src="http://ww2.sinaimg.cn/large/0060lm7Tly1fmzvvgxbw3j308v0ecq44.jpg" alt="figure 1"><br>
Figure 1：一个采用ReLUs（实线）的4层卷积神经网络达到训练误差率25% 6倍快于带有tanh神经元（虚线）的同等网络。每个网络的学习率都是独立地进行选择以求尽可能快地训练。网络中没有采用任何类型的正则化。这里所展示的效果的量级受网络架构的影响而有可能不同，但是具有ReLUs的网络一直是比带有饱和神经元的网络在学习速度上要快上几倍。</p> <p>我们并不是第一个考虑在CNNs中替换传统神经元模型的人。就比如说，Jarrett等人声称，非线性 f(x) = |tanh(x)| 在Caltech-101数据集上与局部平均池化（local average pooling）后的对比归一化的合作表现良好（the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset.）。然而，他们在这数据集上主要关注的是防止过拟合，所以观察到的结果与我们报告中使用ReLUs拟合训练集的加速能力有所不同。更快速的学习能力对在大规模数据集上训练的大型模型的性能具有很大的影响。</p> <h2 id="在多个gpus上训练"><a href="#在多个gpus上训练" class="header-anchor">#</a> 在多个GPUs上训练</h2> <p>单个GTX 580 GPU只有3GB的内存大小，这限制了能够在其上训练的网络的最大规模。事实证明，120万个训练样本足以训练出网络，但这对于单个GPU来说太大了。因此，我们将网络分布到两个GPUs上。当前的GPUs非常适合跨GPU做并行计算，因为它们能够直接向另一个GPU做读取写入操作，而无需通过主机内存中转。我们所采用的的并行化方案基本上是在每个GPU上放置一半的内核（或神经元），另外还有一个技巧：GPUs之间的通信只在某些层中进行。这意味着，比如说，第3层的内核从第2层的所有内核映射中获取输入。然而，第4层的内核只从第3层中和自己在同一个GPU上的内核映射中获取输入。选择连接的模式对于交叉验证来说是一个问题，但我们可以精确地调整通信量，直到它达到计算量的可接受部分为止。</p> <p>由此产生的架构有点类似于Cirespan等人使用的“柱状（columnar）”CNN，只是我们的纵列（columns）不是独立的。与在一个GPU上训练的且每个卷积层内核数量减少一半的网络相比，这个方案将我们的top-1和top-5误差率分别降低了1.7%和1.2%。训练双GPU网络的时间较少于单个GPU网络。</p> <h2 id="局部响应归一化（local-response-normalization）"><a href="#局部响应归一化（local-response-normalization）" class="header-anchor">#</a> 局部响应归一化（Local Response Normalization）</h2> <p>ReLUs有一个特性使得它们不需要输入归一化来防止它们饱和。如果有一些训练样本产生了正输入（positive input）给ReLUs，就会在那个神经元中进行学习操作。然而，我们仍发现下面这种局部归一化方案有助于一般化（generalization）。假设用 α<sup>i</sup><sub>x,y</sub> 表示在 (x, y) 处由第 i 个内核计算而得的神经元的活动，之后应用ReLU非线性，最后响应归一化活动 b<sup>i</sup><sub>x,y</sub> 由以下公式定义：<br> <img src="http://ww1.sinaimg.cn/large/0060lm7Tly1fmzw5ga72qj309p028t8p.jpg" alt="b"><br>
其中求和操作是作用于同一空间位置的n个邻近内核映射上，N是层中内核总数。内核映射的顺序是任意的，且在训练开始前就确定好了的。受到真实神经元中的类型所启发，这种响应归一化实现了一种侧向抑制形式（a form of lateral inhibition），为使用不同内核计算得到的神经元输出中的大型活动（big activities）创建竞争机制。常量 k,n,α,β 是超参数，它们的值通过验证集来确定；我们取k=2, n=5, α=10<sup>-4</sup>, β=0.75 。在特定层应用ReLUs非线性后，我们应用了这种归一化（见Section 3.5）。</p> <p>该方案与Jarrett等人的局部对比归一化（local contrast normalization）方法有一些相似之处，但是我们的方案更应正确地被命名为“亮度归一化（brightness normalization）”，因为我们没有减去平均活动。响应归一化将我们的top-1和top-5误差率分别下降了1.4%和1.2%。我们也在CIFAR-10数据集上验证了该方案的有效性：未采用归一化的4层CNN取得了13%的测试误差率，采用归一化的只有11%。</p> <h2 id="重叠池化（overlapping-pooling）"><a href="#重叠池化（overlapping-pooling）" class="header-anchor">#</a> 重叠池化（Overlapping Pooling）</h2> <p>CNNs中的池化层（Pooling layers）汇总了在同一内核映射中相邻神经元组的输出。传统上，由邻近池化单元汇总的邻近关系不会重叠。更准确地说，一个池化层可以被认为是由间隔 s 像素的池化单元组成的网格，每个网格均汇总出以池化单元的位置为中心的大小为 z x z 的邻域关系。如果我们设定 s=z，我们将得到CNNs中常用的传统的局部池化。如果我们设定 s&lt;z，我们将得到重叠池化。这就是我们在网络中所使用的，其中 s=2， z=3。与无重叠的方案 s=2， z=2 相比，这种方案在产生相同维度的输出时分别将top-1和top-5的误差率降低了0.4%和0.3%。我们还观察到，采用重叠池化训练模型会使模型不易出现过拟合问题。</p> <h2 id="总体架构"><a href="#总体架构" class="header-anchor">#</a> 总体架构</h2> <p>现在我们开始介绍我们CNN的总体架构。如图2所示，网络中包含了8个加权（weights）的层；前5个是卷积层，余下的3个是全连接层。最后一个全连接层的输出被发送给1000-way softmax上，其产生1000个类别标签的分布。我们的网络使得多项式逻辑回归目标（multinomial logistic regression objective）最大化，这相当于最大化了预测分布下训练样本中正确标签的log概率的平均值。</p> <p>第2、4、5个卷积层的内核只连接到同一GPU上前一层的那些内核映射上（见Figure 2）。第3个卷积层的内核连接到第2个卷积层的所有内核映射上。全连接层中的神经元与前一层的所有神经元相连。响应归一化层跟在第1、2个卷积层后面。Section 3.4中描述的那种最大池化层跟在两个响应归一化层和第5个卷积层的后面。ReLUs非线性被应用于每个卷积层的输出和全连接层的输出上。</p> <p>第1个卷积层利用96个大小为 11 x 11 x 3 的内核，采用步长4个像素（同一内核映射中相邻神经元的感受野中心之间的距离），对 224 x 224 x 3 的输入图像做滤波（filter）处理。第2个卷积层将第1个卷积层的（响应归一化、池化后的）输出作为输入，且利用256个大小为 5 x 5 x 48 的内核进行滤波。第3、4、5个卷积层彼此相连，中间没有任何的池化层或归一化层。第3个卷积层有384个大小为 3 x 3 x 256 的内核连接到第2个卷积层的输出上。第4个卷积层有384个大小为 3 x 3 x 192 的内核，第5个卷积层有256个大小为 3 x 3 x 192 的内核。全连接层都各有4096个神经元。</p> <p><img src="http://ww2.sinaimg.cn/large/0060lm7Tly1fmzwbxto54j30ll0a6wgo.jpg" alt="figure 2"><br>
Figure 2：CNN架构的图解，明确地展示出两GPU间的责任划定。一个GPU运行图画中顶部的每层部分，另一个运行图画中底部的每层部分。GPUs间的通信只在某些层中进行。网络的输入是150,528维，网络中剩余层的神经元数量分别是：253,440–186,624–64,896–64,896–43,264–4096–4096–1000。</p> <h1 id="减少过拟合"><a href="#减少过拟合" class="header-anchor">#</a> 减少过拟合</h1> <p>我们的神经网络架构中拥有6000万个参数。虽然ILSVRC的1000个类别使得每个训练样本在从图像映射到标签label时都强加了10bits的约束（impose 10 bits of constraint on the mapping from image to label），但是这不足以在学习如此多的参数情况下而没有大量的过拟合（在学习这么多的参数情况下必定会有过拟合问题）。下面，我们介绍两个对抗过拟合的主要方法。</p> <h2 id="数据增强（data-augmentation）"><a href="#数据增强（data-augmentation）" class="header-anchor">#</a> 数据增强（Data Augmentation）</h2> <p>在图像数据上减少过拟合的最简单和最常见的方法是使用标签保存转换（label-preserving transformations）方法人为地扩大数据集规模。我们采用了两个不同的数据增强方式，两者都允许以少量的计算从原始图像中生成转换图像，所以转换图像不需要存储在硬盘上。在我们的实现中，转换图像是由CPU上的Python代码生成的，而GPU正在训练前一batch的图像。因此这些数据增强方案事实上是计算自由的（computationally free）。</p> <p>第一种数据增强方案包括了生成图像翻译（image translations）和水平反射（horizontal reflections）。为此，我们从 256 x 256 的图像中提取随机的 224 x 224 区块（patches）（和它们的水平反射图像），并在这些提取出来的区块上训练我们的网络。这使得我们的训练集增加了2048倍（（256-224）^2 * 2），尽管由此产生的训练样本相互之间高度关联。若没有这种方案，我们的网络将遭受严重的过拟合问题，这就会迫使我们采用更小规模的网络。在测试过程中，网络通过提取5个 224 x 224 区块（4个角落区块和一个中央区块）和它们的水平反射（因此共有10个区块）来做预测，并且对由网络softmax层对这10个区块的预测值做平均处理。</p> <p>第二种数据增强方案包括了更改训练图像的RGB通道的强度（altering the intensities of the RGB channels）。具体来说，我们在整个ImageNet训练集上的RGB像素值集合上都采用了PCA方法。对于每个训练图像，我们成倍增加已有的主要成分，比例大小为对应特征值乘上一个从均值0标准差0.1的高斯分布中提取的随机变量。因此对于每个RGB图像像素<img src="http://ww4.sinaimg.cn/large/0060lm7Tly1fmzwdxf67gj301g00p0pv.jpg" alt=""><img src="http://ww1.sinaimg.cn/large/0060lm7Tly1fmzwecz5f6j303g00tq2q.jpg" alt="">，我们添加如下：<br> <img src="http://ww2.sinaimg.cn/large/0060lm7Tly1fmzwf21ildj307200vt8j.jpg" alt=""><br>
其中<img src="http://ww2.sinaimg.cn/large/0060lm7Tly1fmzwfww1ccj302900lmwx.jpg" alt="">是RGB像素值的 3 x 3 协方差矩阵的第 i 个特征向量和特征值，α<sub>i</sub>是前面提到的随机变量。每个α<sub>i</sub>对于一个特定的训练图像的所有像素仅被绘制一次，直到该图像被再次用来训练，此时它才会被再次绘制。这种方案近似捕捉到了自然图像的一个重要特性，即，目标身份不会随光照的强度和颜色的变化而改变。该方案将top-1误差率降低了1%。</p> <h2 id="dropout"><a href="#dropout" class="header-anchor">#</a> Dropout</h2> <p>结合众多不同模型的预测是一个减少测试误差的不错的方式，但是这对于大型神经网络来说仍过于昂贵，得花上几天时间来训练。然而，有这么一个高效的模型组合版本，只花费两倍的在单个模型上的训练时间。该最近引入的技术，称为“dropout”，以0.5的概率将每个隐藏神经元的输出置为0。以这种方式被“drop out”的神经元既不对前向传播（forward pass）做贡献也不参与反向传播（backpropagation）。所以每次提交输入时，神经网络都采用不同的架构，但所有的架构共享权值。这种技术降低了神经元复杂的互适应关系，因为一个神经元不能依赖于其他特定神经元的存在。因此，它被迫学习更多健壮的特征，这些特征在与其他神经元的不同随机子集相连时是非常有用的。在测试时，我们使用所有的神经元，但它们的输出乘以0.5，对于获取指数级dropout网络产生的预测分布的几何平均值，这是一种合理的近似（a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks）。</p> <p>我们在Figure 2的前两个全连接层上采用了dropout。若没有dropout，我们的网络会表现出实质性的过拟合。Dropout使收敛所需的迭代次数大致增加了一倍。</p> <h1 id="学习的细节"><a href="#学习的细节" class="header-anchor">#</a> 学习的细节</h1> <p>我们采用了随机梯度下降（stochastic gradient descent）方法，并令batch size = 128，momentum = 0.9，weight decay = 0.0005，对模型进行训练。我们发现这个小量的weight decay对于模型学习是十分重要的。换句话说，这儿的weight decay不仅仅是一个regularizer：它减少了模型的训练误差。权值weight w的更新规则是：<br> <img src="http://ww2.sinaimg.cn/large/0060lm7Tly1fmzwik8p2rj30c1026wef.jpg" alt="w"><br>
其中 i 是迭代索引，v 是momentum变量，ε是学习率，<img src="http://ww1.sinaimg.cn/large/0060lm7Tly1fmzwjp5tgxj302n016jr6.jpg" alt="">是第 i 个batch D<sub>i</sub>上的目标L在 w<sub>i</sub> 处对 w 的偏导数的平均值。</p> <p>我们用一个均值为0、标准差为0.01的高斯分布初始化了每一层的权值。我们将第2、4、5个卷积层和全连接隐层的神经元偏移量biases初始化为常量1。这种初始化通过向ReLUs提供正输入来加速学习的早期阶段。 我们将其余层中的神经元偏差biases初始化为0。</p> <p>我们对所有图层使用了相同的学习率，它是由我们在训练过程中手动调节出来的。我们所遵循的启发式是当验证误差率在当前学习率下不再提升（数字下降）时，就将学习率除以10。学习率初始化为0.01，且在训练终止前下降3次。我们用120万张图像的训练集对网络进行了大约90个周期的训练，在两台NVIDIA GTX 580 3GB GPU上花费了五到六天的时间。</p> <h1 id="结果"><a href="#结果" class="header-anchor">#</a> 结果</h1> <p>关于ILSVRC-2010数据集的结果已总结展示在表1中。我们训练的网络在top-1和top-5测试集误差率上分别是37.5%和17.0%。在ILSVRC-2010比赛中取得的最好结果是47.1%和28.2%，它是通过由不同特征训练出来的6个稀疏编码（sparse-coding）模型产生的预测值求平均而得；之后，所公布的最好结果是45.7%和25.7%，它是在由两种类型密集采样特征（densely-sampled features）计算得到的Fisher向量（Fisher Vectors）上训练出来的分类器的预测值求平均。</p> <p><img src="http://ww2.sinaimg.cn/large/0060lm7Tly1fmzwmlbztij308p04nt94.jpg" alt="table 1"></p> <p>我们同样将我们的模型参加了ILSVRC-2012比赛，其中获得的结果如表2所示。由于ILSVRC-2012测试集的labels并未公开提供，因此我们不能报告展示出我们尝试的所有模型的测试误差率。在本段的剩余部分，我们使用验证误差率来替换测试误差率，因为根据我们的经验，它们之间的差异不会超过0.1%（见表2）。本文介绍的CNN其top-5误差率达到了18.2%。5个相似CNN的top-5误差率平均值是16.4%。训练一个在最后池化层后加上第6个卷积层的CNN，去对整个ImageNet Fall 2011版本数据集（1500万张图像，22000个类别）进行分类，然后对其进行调优，在ILSVRC-2012上的top-5误差率达到16.6%。两个对Fall 2011数据集预训练的CNN，加上前面所提到的5个CNN，对它们的预测值求平均后，top-5误差率达到15.3%。比赛中第二名的top-5达到26.2%，他的方法是在几个不同类型的密度采样特征（densely-sampled features）计算得到的FVs上训练出的分类器的预测值的平均值。</p> <p><img src="http://ww3.sinaimg.cn/large/0060lm7Tly1fmzwncub4cj30cw06et9s.jpg" alt="table 2"></p> <p>最后，我们也展示了在ImageNet Fall 2009版本数据集（10184个类别，890万张图像）上的误差率。在这个数据集上，我们遵循文献中使用一半图像做训练和一半做测试的惯例。由于没有建立测试集，我们的数据集划分应该与之前的作者使用的划分有所不同，但是这并不影响结果。在这数据集上的top-1和top-5误差率达到67.4%和40.9%，该结果是由上述网络（在最后池化层后带有一个额外的第6个卷积层）所得到。在该数据集上已公布的最优结果是78.1%和60.9%。</p> <h2 id="定性评估（qualitative-evaluations）"><a href="#定性评估（qualitative-evaluations）" class="header-anchor">#</a> 定性评估（Qualitative Evaluations）</h2> <p>Figure 3展示了通过网络的两个数据连接层学习得到的卷积内核。该网络已学习了各种频率选择（frequency-selective）和方向选择（orientation-selective）的内核，以及各种色块斑点（colored-blobs）。考虑到由两个GPUs展示的专业化，受限连通性（restricted connectivity）的结果呈现在Section 3.5中。GPU 1上的内核很大程度上与颜色无关（ color-agnostic），而GPU 2上的内核与颜色密切相关（ color-specific）。这种专业化发生在每一次运行当中，并且是独立于任何特定的随机权重初始化（GPUs重新编号）。</p> <p><img src="http://ww3.sinaimg.cn/large/0060lm7Tly1fmzwo7zfw1j3082072tba.jpg" alt="figure 3"><br>
Figure 3：由第1个卷积层在 224 x 224 x 3 输入图像上训练得到的96个大小为 11 x 11 x 3 的卷积内核。上边的48个内核是在GPU 1上学习得到，下边的48个内核是在GPU 2上学习得到。具体内容见Section 6.1。</p> <p><img src="http://ww3.sinaimg.cn/large/0060lm7Tly1fmzwoxitonj30jy0b0h06.jpg" alt="figure 4"><br>
Figure 4：（左）8张ILSVRC-2010测试图像和由模型认为的最有可能的5个标签。正确的标签显示在每张图像的下方，分配给正确标签的概率也用红色框标注显示在上图中（如果正确标签在预测的5个标签当中）。（右）第一列上有5张ILSVRC-2010测试图像。其余列上展示的是与测试图像在最后一个隐层上的特征向量具有最短欧氏距离的6张训练图像。</p> <p>在Figure 4的左半部分，我们通过计算网络模型在8张测试图像上的top-5预测值来定性地评估（网络）学到了什么。请注意，即使是偏离中心位置的目标，比如左上角的那只螨虫，仍被网络所识别出来。大部分top-5的标签（labels）看起来都很合理。比如说，只有一些其他类别的猫科动物被错误认为是豹类。在某些情况下（汽车护栅，樱桃），网络对照片中究竟应关注哪个目标对象存在着歧义。</p> <p>探索网络视觉内容的另一种方法是考虑由最后一个图像（4096维度的隐藏层）引起的特征激活（feature activations）。如果两张图像的特征激活向量具有较小的欧氏距离，则可以说神经网络认为它们在很大程度上相似。Figure 4右半部分中展示了测试集中的5张图像，还有根据上述方法找出来的与这5张图像中每个图像最相似的6张训练集中的图像。请注意，在像素级别上，所检索到的训练图像一般是不与第一列中的查询图像在L2上相近。比如说，所检索的dogs和elephants图像中它们有各种各样的姿势。我们在补充材料里提供了更多的测试图像的结果。</p> <p>在两个4096维的实值向量之间使用欧氏距离来计算相似度的效率十分低，但是通过训练自动编码器（auto-encoder）来将这些向量压缩成短二进制码（short binary codes）将会使之变得高效。这应该会产生出一种比对原始像素应用自编码更好的图像检索方法，因为不需要用到图像标签labels，因此倾向于检索出带有相似边缘模式的图像，无论它们在语义上是否相似。</p> <h1 id="讨论"><a href="#讨论" class="header-anchor">#</a> 讨论</h1> <p>我们的结果展示出：一个大型深度卷积神经网络能够在一个极具挑战的数据集上使用纯监督学习（supervised learning）而取得破纪录的成绩结果。值得注意的是，如果移除一个卷积层，我们网络的性能就会下降。比如，移除任意一个中间层就会导致网络的top-1误差率增加2%。所以深度（depth）对于我们获得结果非常重要。</p> <p>为了简化实验，我们没有使用任何无监督的预训练，即使我们知道这样会有帮助，特别是当我们获得了足够的计算能力去大幅度提升网络的规模而不去对应地增加带有label的数据时。至此，我们的结果已经得到优化，因为我们已让网络规模更大，训练它们的时间更长，但是我们仍有许多数量级（many orders of magnitude）要去做，以求匹配上人类视觉系统的神经-时间通路（infero-temporal pathway）。最后，我们希望在视频序列上运用大型深度卷积神经网络，因为视频序列的时序结构提供了在静态图像中丢失或者不明显的有用信息。</p></div></article> <section class="post-meta main-div" data-v-4e23451f><section class="post-date clearfix" data-v-4e23451f><span class="create-date" data-v-4e23451f>
      Created : 2017-12-31
    </span> <!----></section> <section class="post-links" data-v-4e23451f><a href="/posts/2017/11/19/next-hexo-blog.html" class="post-link" data-v-4e23451f>
      Previous Post : 利用GitHub+Node.js+Hexo搭建个人博客
    </a> <a href="/posts/2018/01/01/life-new-year-2018.html" class="post-link" data-v-4e23451f>
      Next Post : Happy New Year
    </a></section></section> <div id="post-comments" class="main-div"><!----></div></div></main> <aside class="aside" data-v-4dd605a1><div class="info-card main-div" data-v-9d847660 data-v-4dd605a1><div class="info-card-header" data-v-9d847660><img src="/img/avatar.jpg" alt="asdfv1929" class="info-avatar" data-v-9d847660></div> <div class="info-card-body" data-v-9d847660><section class="info-nickname" data-v-9d847660>
      asdfv1929
    </section> <section class="info-desc" data-v-9d847660>Hello World</section> <section class="info-contact" data-v-9d847660><section data-v-9d847660><span title="Modu, China" data-v-9d847660 data-v-9d847660><svg class="icon" style="font-size:1em;" data-v-9d847660 data-v-9d847660><title data-v-9d847660 data-v-9d847660>Modu, China</title><use xlink:href="#icon-location" data-v-9d847660 data-v-9d847660></use></svg><span class="info-text" data-v-9d847660 data-v-9d847660>
          Modu, China
        </span></span></section> <section data-v-9d847660><span title="Modu" data-v-9d847660 data-v-9d847660><svg class="icon" style="font-size:1em;" data-v-9d847660 data-v-9d847660><title data-v-9d847660 data-v-9d847660>Modu</title><use xlink:href="#icon-organization" data-v-9d847660 data-v-9d847660></use></svg><span class="info-text" data-v-9d847660 data-v-9d847660>
          Modu
        </span></span></section> <section data-v-9d847660><a href="mailto:asdfv1929@163.com" title="asdfv1929@163.com" data-v-9d847660 data-v-9d847660><svg class="icon" style="font-size:1em;" data-v-9d847660 data-v-9d847660><title data-v-9d847660 data-v-9d847660>asdfv1929@163.com</title><use xlink:href="#icon-email" data-v-9d847660 data-v-9d847660></use></svg><span class="info-text" data-v-9d847660 data-v-9d847660>
          asdfv1929@163.com
        </span></a></section></section></div> <div class="info-card-footer" data-v-9d847660><section class="info-sns clearfix" data-v-9d847660><a href="https://github.com/asdfv1929" target="_blank" class="sns-link" data-v-9d847660><span title="GitHub: asdfv1929" class="sns-icon" data-v-9d847660 data-v-9d847660><svg class="icon" style="font-size:1.5em;" data-v-9d847660 data-v-9d847660><title data-v-9d847660 data-v-9d847660>GitHub: asdfv1929</title><use xlink:href="#icon-github" data-v-9d847660 data-v-9d847660></use></svg></span></a></section></div></div> <div class="post-nav-card main-div" style="position:relative;top:0;width:0px;" data-v-4dd605a1><div class="post-nav-contents"><svg class="icon"><title>book</title><use xlink:href="#icon-book"></use></svg> <span>Table of Contents</span> <div class="post-nav-toc"><ul><li><a href="/posts/2017/12/31/dl-alexnet.html#relu-非线性（nonlinearity）">ReLU 非线性（Nonlinearity）</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#在多个gpus上训练">在多个GPUs上训练</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#局部响应归一化（local-response-normalization）">局部响应归一化（Local Response Normalization）</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#重叠池化（overlapping-pooling）">重叠池化（Overlapping Pooling）</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#总体架构">总体架构</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#数据增强（data-augmentation）">数据增强（Data Augmentation）</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#dropout">Dropout</a></li><li><a href="/posts/2017/12/31/dl-alexnet.html#定性评估（qualitative-evaluations）">定性评估（Qualitative Evaluations）</a></li></ul></div></div> <div class="post-nav-comments"><svg class="icon"><title>comment</title><use xlink:href="#icon-comment"></use></svg> <a href="/posts/2017/12/31/dl-alexnet.html#post-comments">
      Comments
    </a></div></div></aside></div> <footer class="footer" data-v-1375e54c><p class="footer-sns-links" data-v-1375e54c><a href="https://github.com/asdfv1929" target="_blank" class="sns-link" data-v-1375e54c><span title="GitHub: asdfv1929" class="sns-icon" data-v-1375e54c data-v-1375e54c><svg class="icon" style="font-size:25px;" data-v-1375e54c data-v-1375e54c><title data-v-1375e54c data-v-1375e54c>GitHub: asdfv1929</title><use xlink:href="#icon-github" data-v-1375e54c data-v-1375e54c></use></svg></span></a></p> <p class="footer-text" data-v-1375e54c><span data-v-1375e54c>Powered by </span> <a href="https://github.com/vuejs/vuepress" target="_blank" data-v-1375e54c>
      VuePress
    </a> <span data-v-1375e54c> | </span> <a href="https://github.com/meteorlxy/vuepress-theme-meteorlxy" target="_blank" data-v-1375e54c>
        meteorlxy
      </a></p> <p class="footer-text" data-v-1375e54c>Copyright 2017-present <a href="https://github.com/asdfv1929" target="_blank">asdfv1929</a></p></footer></div><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.806dfd1e.js" defer></script><script src="/assets/js/7.97edb3cd.js" defer></script><script src="/assets/js/17.84ae493d.js" defer></script>
  </body>
</html>
